{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrape LA Apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#import get to call a get request on the site\n",
    "from requests import get\n",
    "\n",
    "#get the first page of the east bay housing prices\n",
    "response = get('https://losangeles.craigslist.org/search/wst/apa') #get rid of those lame-o's that post a housing option without a pic using their filter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#get the macro-container for the housing posts\n",
    "posts = html_soup.find_all('li', class_= 'result-row')\n",
    "print(type(posts)) #to double check that I got a ResultSet\n",
    "print(len(posts)) #to double check I got 120 (elements/page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the first post\n",
    "post_one = posts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$1,799'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#grab the price of the first post\n",
    "post_one_price = post_one.a.text\n",
    "post_one_price.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the time of the post in datetime format to save on cleaning efforts\n",
    "post_one_time = post_one.find('time', class_= 'result-date')\n",
    "post_one_datetime = post_one_time['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title is a and that class, link is grabbing the href attribute of that variable\n",
    "post_one_title = post_one.find('a', class_='result-title hdrlnk')\n",
    "post_one_link = post_one_title['href']\n",
    "\n",
    "#easy to grab the post title by taking the text element of the title variable\n",
    "post_one_title_text = post_one_title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabs the whole segment of housing details. We will need missing value handling in the loop as this kind of detail is not common in posts\n",
    "#the text can be split, and we can use indexing to grab the elements we want. number of bedrooms is the first element.\n",
    "#sqft is the third element\n",
    "\n",
    "post_one_num_bedrooms = post_one.find('span', class_ = 'housing').text.split()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the neighborhood is grabbed by finding the span class 'result-hood' and pulling the text element from that\n",
    "post_one_hood = posts[0].find('span', class_='result-hood').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bf/7kjr62fs6wq3x9hlt0xsr0w40000gn/T/ipykernel_18002/4102016945.py:7: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 scraped successfully!\n",
      "Page 2 scraped successfully!\n",
      "Page 3 scraped successfully!\n",
      "Page 4 scraped successfully!\n",
      "Page 5 scraped successfully!\n",
      "Page 6 scraped successfully!\n",
      "Page 7 scraped successfully!\n",
      "Page 8 scraped successfully!\n",
      "Page 9 scraped successfully!\n",
      "Page 10 scraped successfully!\n",
      "Page 11 scraped successfully!\n",
      "Page 12 scraped successfully!\n",
      "Page 13 scraped successfully!\n",
      "Page 14 scraped successfully!\n",
      "Page 15 scraped successfully!\n",
      "Page 16 scraped successfully!\n",
      "Page 17 scraped successfully!\n",
      "Page 18 scraped successfully!\n",
      "Page 19 scraped successfully!\n",
      "Page 20 scraped successfully!\n",
      "Page 21 scraped successfully!\n",
      "Page 22 scraped successfully!\n",
      "Page 23 scraped successfully!\n",
      "Page 24 scraped successfully!\n",
      "Page 25 scraped successfully!\n",
      "Page 26 scraped successfully!\n",
      "\n",
      "\n",
      "Scrape complete!\n"
     ]
    }
   ],
   "source": [
    "#build out the loop\n",
    "from time import sleep\n",
    "import re\n",
    "from random import randint #avoid throttling by not sending too many requests one after the other\n",
    "from warnings import warn\n",
    "from time import time\n",
    "from IPython.core.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "#find the total number of posts to find the limit of the pagination\n",
    "results_num = html_soup.find('div', class_= 'search-legend')\n",
    "results_total = int(results_num.find('span', class_='totalcount').text) #pulled the total count of posts as the upper bound of the pages array\n",
    "\n",
    "#each page has 119 posts so each new page is defined as follows: s=120, s=240, s=360, and so on. So we need to step in size 120 in the np.arange function\n",
    "pages = np.arange(0, results_total+1, 120)\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "post_timing = []\n",
    "post_hoods = []\n",
    "post_title_texts = []\n",
    "bedroom_counts = []\n",
    "sqfts = []\n",
    "post_links = []\n",
    "post_prices = []\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    #get request\n",
    "    response = get(\"https://losangeles.craigslist.org/search/wst/apa?\" \n",
    "                   + \"s=\" #the parameter for defining the page number \n",
    "                   + str(page) #the page number in the pages array from earlier\n",
    "                   + \"&hasPic=1\"\n",
    "                   + \"&availabilityMode=0\")\n",
    "\n",
    "    sleep(randint(1,5))\n",
    "     \n",
    "    #throw warning for status codes that are not 200\n",
    "    if response.status_code != 200:\n",
    "        warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "        \n",
    "    #define the html text\n",
    "    page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #define the posts\n",
    "    posts = html_soup.find_all('li', class_= 'result-row')\n",
    "        \n",
    "    #extract data item-wise\n",
    "    for post in posts:\n",
    "\n",
    "        if post.find('span', class_ = 'result-hood') is not None:\n",
    "\n",
    "            #posting date\n",
    "            #grab the datetime element 0 for date and 1 for time\n",
    "            post_datetime = post.find('time', class_= 'result-date')['datetime']\n",
    "            post_timing.append(post_datetime)\n",
    "\n",
    "            #neighborhoods\n",
    "            post_hood = post.find('span', class_= 'result-hood').text\n",
    "            post_hoods.append(post_hood)\n",
    "\n",
    "            #title text\n",
    "            post_title = post.find('a', class_='result-title hdrlnk')\n",
    "            post_title_text = post_title.text\n",
    "            post_title_texts.append(post_title_text)\n",
    "\n",
    "            #post link\n",
    "            post_link = post_title['href']\n",
    "            post_links.append(post_link)\n",
    "            \n",
    "            #removes the \\n whitespace from each side, removes the currency symbol, and turns it into an int\n",
    "            post_price = post.a.text.strip().replace(\"$\", \"\")\n",
    "            post_prices.append(post_price)\n",
    "            \n",
    "            if post.find('span', class_ = 'housing') is not None:\n",
    "                \n",
    "                #if the first element is accidentally square footage\n",
    "                if 'ft2' in post.find('span', class_ = 'housing').text.split()[0]:\n",
    "                    \n",
    "                    #make bedroom nan\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #make sqft the first element\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[0][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if the length of the housing details element is more than 2\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) > 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = int(post.find('span', class_ = 'housing').text.split()[2][:-3])\n",
    "                    sqfts.append(sqft)\n",
    "                    \n",
    "                #if there is num bedrooms but no sqft\n",
    "                elif len(post.find('span', class_ = 'housing').text.split()) == 2:\n",
    "                    \n",
    "                    #therefore element 0 will be bedroom count\n",
    "                    bedroom_count = post.find('span', class_ = 'housing').text.replace(\"br\", \"\").split()[0]\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                    \n",
    "                    #and sqft will be number 3, so set these here and append\n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)                    \n",
    "                \n",
    "                else:\n",
    "                    bedroom_count = np.nan\n",
    "                    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                    sqft = np.nan\n",
    "                    sqfts.append(sqft)\n",
    "                \n",
    "            #if none of those conditions catch, make bedroom nan, this won't be needed    \n",
    "            else:\n",
    "                bedroom_count = np.nan\n",
    "                bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "                sqft = np.nan\n",
    "                sqfts.append(sqft)\n",
    "            #    bedroom_counts.append(bedroom_count)\n",
    "                \n",
    "            #    sqft = np.nan\n",
    "            #    sqfts.append(sqft)\n",
    "                \n",
    "    iterations += 1\n",
    "    print(\"Page \" + str(iterations) + \" scraped successfully!\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Scrape complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "la_apts_wst = pd.DataFrame({'posted': post_timing,\n",
    "                       'neighborhood': post_hoods,\n",
    "                       'post title': post_title_texts,\n",
    "                       'number bedrooms': bedroom_counts,\n",
    "                        'sqft': sqfts,\n",
    "                        'URL': post_links,\n",
    "                       'price': post_prices})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posted</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>post title</th>\n",
       "      <th>number bedrooms</th>\n",
       "      <th>sqft</th>\n",
       "      <th>URL</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-22 09:51</td>\n",
       "      <td>(SOCAL)</td>\n",
       "      <td>Completely remodeled 3bd property</td>\n",
       "      <td>3</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>https://losangeles.craigslist.org/wst/apa/d/co...</td>\n",
       "      <td>1,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-22 09:50</td>\n",
       "      <td>(San Pedro)</td>\n",
       "      <td>#718 View Sunday, Pets Welcome, Ideal San Pedr...</td>\n",
       "      <td>1</td>\n",
       "      <td>548.0</td>\n",
       "      <td>https://losangeles.craigslist.org/wst/apa/d/sa...</td>\n",
       "      <td>2,330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-22 09:50</td>\n",
       "      <td>(westside-southbay-310)</td>\n",
       "      <td>Must See 1 Bedroom in Beverly Grove | Granite ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://losangeles.craigslist.org/wst/apa/d/lo...</td>\n",
       "      <td>2,295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-22 09:48</td>\n",
       "      <td>(westside-southbay-310)</td>\n",
       "      <td>In Unit Washer/Dryer</td>\n",
       "      <td>1</td>\n",
       "      <td>687.0</td>\n",
       "      <td>https://losangeles.craigslist.org/wst/apa/d/lo...</td>\n",
       "      <td>2,455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-22 09:48</td>\n",
       "      <td>(415 Gayley Ave., Los Angeles, CA)</td>\n",
       "      <td>This Is What You’ve Been Searching For. Pre-Le...</td>\n",
       "      <td>1</td>\n",
       "      <td>650.0</td>\n",
       "      <td>https://losangeles.craigslist.org/wst/apa/d/lo...</td>\n",
       "      <td>2,656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             posted                         neighborhood  \\\n",
       "0  2022-05-22 09:51                              (SOCAL)   \n",
       "1  2022-05-22 09:50                          (San Pedro)   \n",
       "2  2022-05-22 09:50              (westside-southbay-310)   \n",
       "3  2022-05-22 09:48              (westside-southbay-310)   \n",
       "4  2022-05-22 09:48   (415 Gayley Ave., Los Angeles, CA)   \n",
       "\n",
       "                                          post title number bedrooms    sqft  \\\n",
       "0                  Completely remodeled 3bd property               3  1104.0   \n",
       "1  #718 View Sunday, Pets Welcome, Ideal San Pedr...               1   548.0   \n",
       "2  Must See 1 Bedroom in Beverly Grove | Granite ...               1     NaN   \n",
       "3                               In Unit Washer/Dryer               1   687.0   \n",
       "4  This Is What You’ve Been Searching For. Pre-Le...               1   650.0   \n",
       "\n",
       "                                                 URL  price  \n",
       "0  https://losangeles.craigslist.org/wst/apa/d/co...  1,799  \n",
       "1  https://losangeles.craigslist.org/wst/apa/d/sa...  2,330  \n",
       "2  https://losangeles.craigslist.org/wst/apa/d/lo...  2,295  \n",
       "3  https://losangeles.craigslist.org/wst/apa/d/lo...  2,455  \n",
       "4  https://losangeles.craigslist.org/wst/apa/d/lo...  2,656  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la_apts_wst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_apts_wst.to_csv('la_apts_wst.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
